{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61642abf-fc11-43c7-a87e-d3b32b649e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4f6a87f-db6e-43cc-b158-3ec4848ba6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c50f8-0cf7-4211-ae54-a8c3bf7db1eb",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60bb1067-253f-405a-a1fe-534e99a69ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────── Hyperparameters ───────\n",
    "SEQ_LEN     = 60        # Length of input sequence (seconds)\n",
    "BATCH_SIZE  = 64        # Batch size for training\n",
    "HIDDEN_SIZE = 64        # Number of hidden units in LSTM\n",
    "NUM_LAYERS  = 2         # Number of LSTM layers\n",
    "DROPOUT     = 0.2       # Dropout rate between LSTM layers\n",
    "LR          = 1e-3      # Learning rate for Adam optimizer\n",
    "MAX_EPOCHS  = 50        # Maximum number of training epochs\n",
    "PATIENCE    = 7         # Early stopping patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2aa5c62-f6f5-485a-88be-fe24382e7255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Train samples: 176281 Test samples: 50281\n"
     ]
    }
   ],
   "source": [
    "FEATURES = [\n",
    "    \"Ambient Temp Trace(F)\",\n",
    "    \"Inlet Temp Trace(F)\",\n",
    "    \"Outlet Temp Trace (F)\",\n",
    "    \"Operational Mode Code\",\n",
    "]\n",
    "TARGET = \"Power (W)\"\n",
    "\n",
    "# ─────── Device configuration ───────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ─────── Load and scale data ───────\n",
    "train_df = pd.read_csv(r\"C:\\Users\\jliu359\\Downloads\\Hpwh\\Train\\Train\\Merged_HPWH_Train_1s.csv\")\n",
    "test_df  = pd.read_csv(r\"C:\\Users\\jliu359\\Downloads\\Hpwh\\Test\\Test\\Merged_HPWH_Test_1s.csv\")\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Fit scalers on training set and transform both train and test\n",
    "X_train = scaler_x.fit_transform(train_df[FEATURES])\n",
    "y_train = scaler_y.fit_transform(train_df[[TARGET]]).squeeze(-1)\n",
    "X_test  = scaler_x.transform(test_df[FEATURES])\n",
    "y_test  = scaler_y.transform(test_df[[TARGET]]).squeeze(-1)\n",
    "\n",
    "# ─────── Dataset definition (lazy slicing) ───────\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len):\n",
    "        # Store as tensors; slicing will happen in __getitem__\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of samples is total length minus one sequence\n",
    "        return self.X.size(0) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return one sequence window and its target value\n",
    "        x_seq = self.X[idx : idx + self.seq_len]  # Shape: [seq_len, num_features]\n",
    "        y_val = self.y[idx + self.seq_len]        # Scalar target\n",
    "        return x_seq, y_val\n",
    "\n",
    "# Instantiate datasets and loaders\n",
    "train_ds = SequenceDataset(X_train, y_train, SEQ_LEN)\n",
    "test_ds  = SequenceDataset(X_test,  y_test,  SEQ_LEN)\n",
    "print(\"Train samples:\", len(train_ds), \"Test samples:\", len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ddb06-9d90-44ce-a478-47923967d593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE: 0.02831 | Val MSE: 0.06213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | Train MSE: 0.02184 | Val MSE: 0.05547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | Train MSE: 0.01825 | Val MSE: 0.09964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | Train MSE: 0.01120 | Val MSE: 0.12617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50:  10%|█         | 280/2755 [00:10<01:30, 27.31it/s, loss=0.000816]"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True\n",
    ")\n",
    "\n",
    "# ─────── Model definition ───────\n",
    "class HPWH_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        # Stacked LSTM with dropout between layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        # Final linear layer to map hidden state to output\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_len, features]\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        # Use the last layer's final hidden state\n",
    "        out = self.fc(h_n[-1])\n",
    "        return out.squeeze(-1)  # Shape: [batch]\n",
    "\n",
    "# Create model, loss, and optimizer\n",
    "model = HPWH_LSTM(\n",
    "    input_size=len(FEATURES),\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ─────── Training loop with early stopping ───────\n",
    "best_val_loss = float(\"inf\")\n",
    "no_improve    = 0\n",
    "\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch}/{MAX_EPOCHS}\", leave=False)\n",
    "    for Xb, yb in loop:\n",
    "        # Move batch to the correct device\n",
    "        Xb, yb = Xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(Xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * Xb.size(0)\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in test_loader:\n",
    "            Xb, yb = Xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "            preds = model(Xb)\n",
    "            val_loss += criterion(preds, yb).item() * Xb.size(0)\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train MSE: {train_loss:.5f} | Val MSE: {val_loss:.5f}\")\n",
    "        # ---- Early stopping check ----\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_hpwh_lstm.pt\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch} (no improvement for {PATIENCE} epochs).\")\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d356fa1c-35c8-442d-9105-1ea82502df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────── Load best model & evaluate ───────\n",
    "model.load_state_dict(torch.load(\"best_hpwh_lstm.pt\"))\n",
    "model.eval()\n",
    "\n",
    "y_true_list, y_pred_list = [], []\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in test_loader:\n",
    "        Xb = Xb.to(device)\n",
    "        preds = model(Xb).cpu().numpy()  # Move to CPU before converting\n",
    "        y_true_list.append(yb.numpy())\n",
    "        y_pred_list.append(preds)\n",
    "\n",
    "# Concatenate batches and inverse-transform\n",
    "y_true_scaled = np.concatenate(y_true_list).reshape(-1, 1)\n",
    "y_pred_scaled = np.concatenate(y_pred_list).reshape(-1, 1)\n",
    "\n",
    "y_true = scaler_y.inverse_transform(y_true_scaled).ravel()\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled).ravel()\n",
    "\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "print(f\"\\nTest MAE: {mae:.2f} W | RMSE: {rmse:.2f} W\")\n",
    "\n",
    "# ─────── Plot predictions vs. actual ───────\n",
    "plt.figure()\n",
    "plt.plot(y_pred, label=\"Predicted\")\n",
    "plt.plot(y_true, label=\"Actual\")\n",
    "plt.legend()\n",
    "plt.title(\"LSTM HPWH Power Prediction\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Power (W)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3fb447-3e7c-46b5-97cb-790808b3eef7",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a0661-52ad-489c-ac30-ee8968809cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hpwh_gru.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error\n",
    "\n",
    "# ───── Hyperparameters ─────\n",
    "SEQ_LEN     = 60\n",
    "BATCH_SIZE  = 64\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS  = 2\n",
    "DROPOUT     = 0.2\n",
    "LR          = 1e-3\n",
    "MAX_EPOCHS  = 50\n",
    "PATIENCE    = 7\n",
    "\n",
    "FEATURES = [\n",
    "    \"Ambient Temp Trace(F)\",\n",
    "    \"Inlet Temp Trace(F)\",\n",
    "    \"Outlet Temp Trace (F)\",\n",
    "    \"Operational Mode Code\",\n",
    "]\n",
    "TARGET = \"Power (W)\"\n",
    "\n",
    "# ───── Device ─────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ───── Load & Scale ─────\n",
    "train_df = pd.read_csv(r\"C:\\Users\\default.DESKTOP-C4C7JDR\\Downloads\\Trian\\Trian\\Merged_HPWH_Train_1s.csv\")\n",
    "test_df  = pd.read_csv(r\"C:\\Users\\default.DESKTOP-C4C7JDR\\Downloads\\Test\\Test\\Merged_HPWH_Test_1s.csv\")\n",
    "\n",
    "scaler_x = MinMaxScaler(); scaler_y = MinMaxScaler()\n",
    "X_train = scaler_x.fit_transform(train_df[FEATURES])\n",
    "y_train = scaler_y.fit_transform(train_df[[TARGET]]).squeeze(-1)\n",
    "X_test  = scaler_x.transform(test_df[FEATURES])\n",
    "y_test  = scaler_y.transform(test_df[[TARGET]]).squeeze(-1)\n",
    "\n",
    "# ───── Dataset ─────\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.seq_len = seq_len\n",
    "    def __len__(self):\n",
    "        return self.X.size(0) - self.seq_len\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.X[idx:idx+self.seq_len],   # [seq_len, features]\n",
    "            self.y[idx+self.seq_len]        # scalar\n",
    "        )\n",
    "\n",
    "train_ds = SequenceDataset(X_train, y_train, SEQ_LEN)\n",
    "test_ds  = SequenceDataset(X_test,  y_test,  SEQ_LEN)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "# ───── Model ─────\n",
    "class HPWH_GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size, hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, features]\n",
    "        _, h_n = self.gru(x)\n",
    "        # h_n: [num_layers, batch, hidden_size]\n",
    "        out = self.fc(h_n[-1])           # use last layer\n",
    "        return out.squeeze(-1)           # [batch]\n",
    "\n",
    "model = HPWH_GRU(\n",
    "    input_size=len(FEATURES),\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ───── Training Loop ─────\n",
    "best_val = float('inf'); wait = 0\n",
    "for epoch in range(1, MAX_EPOCHS+1):\n",
    "    model.train(); train_loss = 0.\n",
    "    for Xb, yb in tqdm(train_loader, desc=f\"GRU Epoch {epoch}/{MAX_EPOCHS}\", leave=False):\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(Xb)\n",
    "        loss  = criterion(preds, yb)\n",
    "        loss.backward(); optimizer.step()\n",
    "        train_loss += loss.item() * Xb.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    model.eval(); val_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in test_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            val_loss += criterion(model(Xb), yb).item() * Xb.size(0)\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"[GRU] Epoch {epoch:02d} | Train MSE: {train_loss:.5f} | Val MSE: {val_loss:.5f}\")\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss; wait = 0\n",
    "        torch.save(model.state_dict(), \"best_gru.pt\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= PATIENCE:\n",
    "            print(\"Early stopping GRU.\")\n",
    "            break\n",
    "\n",
    "# ───── Evaluate ─────\n",
    "model.load_state_dict(torch.load(\"best_gru.pt\"))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in test_loader:\n",
    "        Xb = Xb.to(device)\n",
    "        out = model(Xb).cpu().numpy()\n",
    "        y_pred.append(out); y_true.append(yb.numpy())\n",
    "y_true = scaler_y.inverse_transform(np.concatenate(y_true).reshape(-1,1)).ravel()\n",
    "y_pred = scaler_y.inverse_transform(np.concatenate(y_pred).reshape(-1,1)).ravel()\n",
    "mae  = mean_absolute_error(y_true,y_pred)\n",
    "rmse = root_mean_squared_error(y_true,y_pred)\n",
    "print(f\"GRU Test MAE: {mae:.2f} W | RMSE: {rmse:.2f} W\")\n",
    "\n",
    "# ───── Plot ─────\n",
    "plt.figure()\n",
    "plt.plot(y_pred, label=\"Pred\")\n",
    "plt.plot(y_true, label=\"True\")\n",
    "plt.title(\"GRU HPWH Prediction\")\n",
    "plt.xlabel(\"Time Step\"); plt.ylabel(\"Power (W)\")\n",
    "plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a81edda-3dc0-4c61-8e2c-4fb580c2f863",
   "metadata": {},
   "source": [
    "### 1D_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f2be1-6d80-4884-ae2e-c901430f6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hpwh_cnn.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error\n",
    "\n",
    "# ───── Hyperparameters ─────\n",
    "SEQ_LEN     = 60\n",
    "BATCH_SIZE  = 64\n",
    "NUM_FILTERS = 64\n",
    "KERNEL_SIZE = 3\n",
    "DROPOUT     = 0.2\n",
    "LR          = 1e-3\n",
    "MAX_EPOCHS  = 50\n",
    "PATIENCE    = 7\n",
    "\n",
    "FEATURES = [\n",
    "    \"Ambient Temp Trace(F)\",\n",
    "    \"Inlet Temp Trace(F)\",\n",
    "    \"Outlet Temp Trace (F)\",\n",
    "    \"Operational Mode Code\",\n",
    "]\n",
    "TARGET = \"Power (W)\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ───── Load & Scale ─────\n",
    "train_df = pd.read_csv(r\"C:\\Users\\default.DESKTOP-C4C7JDR\\Downloads\\Trian\\Trian\\Merged_HPWH_Train_1s.csv\")\n",
    "test_df  = pd.read_csv(r\"C:\\Users\\default.DESKTOP-C4C7JDR\\Downloads\\Test\\Test\\Merged_HPWH_Test_1s.csv\")\n",
    "\n",
    "scaler_x = MinMaxScaler(); scaler_y = MinMaxScaler()\n",
    "X_train = scaler_x.fit_transform(train_df[FEATURES])\n",
    "y_train = scaler_y.fit_transform(train_df[[TARGET]]).squeeze(-1)\n",
    "X_test  = scaler_x.transform(test_df[FEATURES])\n",
    "y_test  = scaler_y.transform(test_df[[TARGET]]).squeeze(-1)\n",
    "\n",
    "# ───── Dataset ─────\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.seq_len = seq_len\n",
    "    def __len__(self):\n",
    "        return self.X.size(0) - self.seq_len\n",
    "    def __getitem__(self, idx):\n",
    "        # CNN expects [batch, channels, seq_len]\n",
    "        x = self.X[idx:idx+self.seq_len].transpose(0,1)  # [features, seq_len]\n",
    "        y = self.y[idx+self.seq_len]\n",
    "        return x, y\n",
    "\n",
    "train_dl = DataLoader(SeqDataset(X_train,y_train,SEQ_LEN), batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "test_dl  = DataLoader(SeqDataset(X_test,y_test,SEQ_LEN),  batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "# ───── Model ─────\n",
    "class HPWH_CNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_filters, kernel_size, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, num_filters, kernel_size, padding=kernel_size//2)\n",
    "        self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size, padding=kernel_size//2)\n",
    "        self.pool  = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc    = nn.Linear(num_filters, 1)\n",
    "    def forward(self, x):\n",
    "        # x: [batch, features, seq_len]\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x).squeeze(-1)  # [batch, num_filters]\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x).squeeze(-1)\n",
    "\n",
    "model = HPWH_CNN(\n",
    "    in_channels=len(FEATURES),\n",
    "    num_filters=NUM_FILTERS,\n",
    "    kernel_size=KERNEL_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ───── Train & Early Stop ─────\n",
    "best_val, wait = float('inf'), 0\n",
    "for epoch in range(1, MAX_EPOCHS+1):\n",
    "    model.train(); tloss=0.\n",
    "    for Xb, yb in tqdm(train_dl, desc=f\"CNN Ep {epoch}\", leave=False):\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(Xb)\n",
    "        loss=criterion(out,yb)\n",
    "        loss.backward(); optimizer.step()\n",
    "        tloss+=loss.item()*Xb.size(0)\n",
    "    tloss/=len(train_dl.dataset)\n",
    "\n",
    "    model.eval(); vloss=0.\n",
    "    with torch.no_grad():\n",
    "        for Xb,yb in test_dl:\n",
    "            Xb,yb=Xb.to(device),yb.to(device)\n",
    "            vloss+=criterion(model(Xb),yb).item()*Xb.size(0)\n",
    "    vloss/=len(test_dl.dataset)\n",
    "\n",
    "    print(f\"[CNN] Ep{epoch:02d} Train MSE:{tloss:.5f} Val MSE:{vloss:.5f}\")\n",
    "    if vloss<best_val: best_val,wait=vloss,0; torch.save(model.state_dict(),\"best_cnn.pt\")\n",
    "    else:\n",
    "        wait+=1\n",
    "        if wait>=PATIENCE:\n",
    "            print(\"Early stopping CNN.\"); break\n",
    "\n",
    "# ───── Evaluate ─────\n",
    "model.load_state_dict(torch.load(\"best_cnn.pt\")); model.eval()\n",
    "yt,yp=[],[]\n",
    "with torch.no_grad():\n",
    "    for Xb,yb in test_dl:\n",
    "        out = model(Xb.to(device)).cpu().numpy()\n",
    "        yp.append(out); yt.append(yb.numpy())\n",
    "yt = scaler_y.inverse_transform(np.concatenate(yt).reshape(-1,1)).ravel()\n",
    "yp = scaler_y.inverse_transform(np.concatenate(yp).reshape(-1,1)).ravel()\n",
    "mae=mean_absolute_error(yt,yp); rmse=root_mean_squared_error(yt,yp)\n",
    "print(f\"CNN Test MAE:{mae:.2f} W RMSE:{rmse:.2f} W\")\n",
    "\n",
    "plt.figure(); plt.plot(yp,label=\"Pred\"); plt.plot(yt,label=\"True\")\n",
    "plt.title(\"CNN HPWH\"); plt.xlabel(\"Step\"); plt.ylabel(\"W\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986bf52f-fc94-4450-b5a5-cfe6e468e541",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fdbcb-384e-45b3-bdac-20fa66996a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hpwh_transformer.py\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error\n",
    "\n",
    "# ───── Hyperparams ─────\n",
    "SEQ_LEN     = 60\n",
    "BATCH_SIZE  = 64\n",
    "D_MODEL     = 64    # embedding dim\n",
    "N_HEAD      = 4\n",
    "NUM_LAYERS  = 2\n",
    "DIM_FF      = 128\n",
    "DROPOUT     = 0.1\n",
    "LR          = 1e-3\n",
    "MAX_EPOCHS  = 50\n",
    "PATIENCE    = 7\n",
    "\n",
    "FEATURES = [\n",
    "    \"Ambient Temp Trace(F)\",\n",
    "    \"Inlet Temp Trace(F)\",\n",
    "    \"Outlet Temp Trace (F)\",\n",
    "    \"Operational Mode Code\",\n",
    "]\n",
    "TARGET = \"Power (W)\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ───── Load & Scale ─────\n",
    "train_df = pd.read_csv(r\"C:\\Users\\default.DESKTOP-C4C7JDR\\Downloads\\Trian\\Trian\\Merged_HPWH_Train_1s.csv\")\n",
    "test_df  = pd.read_csv(r\"C:\\Users\\default.DESKTOP-C4C7JDR\\Downloads\\Test\\Test\\Merged_HPWH_Test_1s.csv\")\n",
    "\n",
    "scaler_x = MinMaxScaler(); scaler_y = MinMaxScaler()\n",
    "X_train = scaler_x.fit_transform(train_df[FEATURES])\n",
    "y_train = scaler_y.fit_transform(train_df[[TARGET]]).squeeze(-1)\n",
    "X_test  = scaler_x.transform(test_df[FEATURES])\n",
    "y_test  = scaler_y.transform(test_df[[TARGET]]).squeeze(-1)\n",
    "\n",
    "# ───── Dataset ─────\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.seq_len = seq_len\n",
    "    def __len__(self):\n",
    "        return self.X.size(0)-self.seq_len\n",
    "    def __getitem__(self, idx):\n",
    "        # [seq_len, features]\n",
    "        return self.X[idx:idx+self.seq_len], self.y[idx+self.seq_len]\n",
    "\n",
    "train_dl = DataLoader(SeqDataset(X_train,y_train,SEQ_LEN), batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "test_dl  = DataLoader(SeqDataset(X_test,y_test,SEQ_LEN),  batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "# ───── Positional Encoding ─────\n",
    "class PosEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model,2)*(-math.log(10000.0)/d_model))\n",
    "        pe[:,0::2] = torch.sin(pos*div)\n",
    "        pe[:,1::2] = torch.cos(pos*div)\n",
    "        self.pe = pe.unsqueeze(0)  # [1,max_len,d_model]\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        return x + self.pe[:,:x.size(1),:].to(x.device)\n",
    "\n",
    "# ───── Model ─────\n",
    "class HPWH_Transformer(nn.Module):\n",
    "    def __init__(self, feature_size, d_model, nhead, num_layers, dim_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(feature_size, d_model)\n",
    "        self.pos_enc    = PosEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, features]\n",
    "        x = self.input_proj(x)            # → [batch, seq_len, d_model]\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.transformer(x)           # → [batch, seq_len, d_model]\n",
    "        out = self.fc(x[:,-1,:])          # take last time step\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "model = HPWH_Transformer(\n",
    "    feature_size=len(FEATURES),\n",
    "    d_model=D_MODEL,\n",
    "    nhead=N_HEAD,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dim_ff=DIM_FF,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ───── Train & Early Stop ─────\n",
    "best_val, wait = float('inf'), 0\n",
    "for epoch in range(1, MAX_EPOCHS+1):\n",
    "    model.train(); tloss=0.\n",
    "    for Xb,yb in tqdm(train_dl, desc=f\"Trans Ep{epoch}\", leave=False):\n",
    "        Xb,yb = Xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(Xb)\n",
    "        loss=criterion(out,yb)\n",
    "        loss.backward(); optimizer.step()\n",
    "        tloss+=loss.item()*Xb.size(0)\n",
    "    tloss/=len(train_dl.dataset)\n",
    "\n",
    "    model.eval(); vloss=0.\n",
    "    with torch.no_grad():\n",
    "        for Xb,yb in test_dl:\n",
    "            Xb,yb = Xb.to(device), yb.to(device)\n",
    "            vloss+=criterion(model(Xb),yb).item()*Xb.size(0)\n",
    "    vloss/=len(test_dl.dataset)\n",
    "\n",
    "    print(f\"[Trans] Ep{epoch:02d} Train MSE:{tloss:.5f} Val MSE:{vloss:.5f}\")\n",
    "    if vloss<best_val: best_val,wait=vloss,0; torch.save(model.state_dict(),\"best_trans.pt\")\n",
    "    else:\n",
    "        wait+=1\n",
    "        if wait>=PATIENCE:\n",
    "            print(\"Early stopping Transformer.\"); break\n",
    "\n",
    "# ───── Evaluate ─────\n",
    "model.load_state_dict(torch.load(\"best_trans.pt\")); model.eval()\n",
    "yt,yp=[],[]\n",
    "with torch.no_grad():\n",
    "    for Xb,yb in test_dl:\n",
    "        out = model(Xb.to(device)).cpu().numpy()\n",
    "        yp.append(out); yt.append(yb.numpy())\n",
    "yt = scaler_y.inverse_transform(np.concatenate(yt).reshape(-1,1)).ravel()\n",
    "yp = scaler_y.inverse_transform(np.concatenate(yp).reshape(-1,1)).ravel()\n",
    "mae=mean_absolute_error(yt,yp); rmse=root_mean_squared_error(yt,yp)\n",
    "print(f\"Trans Test MAE:{mae:.2f} W RMSE:{rmse:.2f} W\")\n",
    "\n",
    "plt.figure(); plt.plot(yp,label=\"Pred\"); plt.plot(yt,label=\"True\")\n",
    "plt.title(\"Transformer HPWH\"); plt.xlabel(\"Step\"); plt.ylabel(\"W\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447cdc1-0db0-4bc8-9b89-2e2b1590b0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd82829-7c67-4c4f-b388-845a4a785d13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
